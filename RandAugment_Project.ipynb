{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RandAugment_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM08oK3Cpocv2nUQ6gJY3Ax",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomger2/RandAugment_Course_Project/blob/main/RandAugment_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbQ9KPsWtvGW",
        "outputId": "5b8783cc-849f-4260-cc43-f2bab735fba9"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds\n",
        "from imgaug import augmenters as iaa\n",
        "import imgaug as ia\n",
        "from tensorflow.keras import metrics\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix as cx\n",
        "from sklearn.metrics import classification_report\n",
        "from scipy.stats import friedmanchisquare\n",
        "\n",
        "np.random.seed(1)\n",
        "tfds.disable_progress_bar()\n",
        "tf.random.set_seed(42)\n",
        "ia.seed(42)\n",
        "\n",
        "!pip install imgaug --upgrade\n",
        "print (ia.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: imgaug in /usr/local/lib/python3.7/dist-packages (0.4.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from imgaug) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from imgaug) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug) (0.16.2)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: opencv-python in /usr/local/lib/python3.7/dist-packages (from imgaug) (4.1.2.30)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from imgaug) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug) (1.7.1)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug) (2.5.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug) (4.4.2)\n",
            "0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyVphU-tuOoa"
      },
      "source": [
        "AUTO = tf.data.AUTOTUNE\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "IMAGE_SIZE = 72\n",
        "NUM_FOLDS = 10\n",
        "\n",
        "CURRENT_METHOD = 'AUTOAUGMENT' #NORMAL / AUTOAUGMENT / COMBINED\n",
        "\n",
        "CURRENT_DATASET = 'cifar10' #cifar10 / cifar100 / mnist / fashion\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxDQWzmtuSQt"
      },
      "source": [
        "rand_aug = iaa.RandAugment(n=5, m=5)\n",
        "\n",
        "\n",
        "simple_aug = tf.keras.Sequential(\n",
        "    [\n",
        "        layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
        "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "        layers.experimental.preprocessing.RandomRotation(factor=0.02),\n",
        "        layers.experimental.preprocessing.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def augment(images):\n",
        "    # Input to `augment()` is a TensorFlow tensor which\n",
        "    # is not supported by `imgaug`. This is why we first\n",
        "    # convert it to its `numpy` variant.\n",
        "    images = tf.cast(images, tf.uint8)\n",
        "    return rand_aug(images=images.numpy())\n",
        "\n",
        "\n",
        "def get_training_model(augmentation):\n",
        "    resnet50_v2 = tf.keras.applications.ResNet50V2(\n",
        "        weights=None,\n",
        "        include_top=True,\n",
        "        input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n",
        "        classes=10,\n",
        "    )\n",
        "    model = tf.keras.Sequential(\n",
        "        [\n",
        "            augmentation,\n",
        "            layers.InputLayer((IMAGE_SIZE, IMAGE_SIZE, 3)),\n",
        "            layers.experimental.preprocessing.Rescaling(scale=1.0 / 127.5, offset=-1),\n",
        "            resnet50_v2,\n",
        "        ]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_basic_training_model():\n",
        "  resnet50_v2 = tf.keras.applications.ResNet50V2(\n",
        "        weights=None,\n",
        "        include_top=True,\n",
        "        input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n",
        "        classes=10,\n",
        "    )\n",
        "  model = tf.keras.Sequential(\n",
        "        [\n",
        "            layers.InputLayer((IMAGE_SIZE, IMAGE_SIZE, 3)),\n",
        "            layers.experimental.preprocessing.Rescaling(scale=1.0 / 127.5, offset=-1),\n",
        "            resnet50_v2,\n",
        "        ]\n",
        "    )\n",
        "  return model\n",
        "\n",
        "\n",
        "def frideman_test(data):\n",
        "  stat,p = friedmanchisquare(data)\n",
        "  alpha = 0.05\n",
        "  if p > alpha:\n",
        "    print('same distribution')\n",
        "  else:\n",
        "    print('diffrent distribution')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFwrErd0uKjL",
        "outputId": "18bbed2e-0d70-41f6-f3aa-d01956fa23c8"
      },
      "source": [
        "if CURRENT_DATASET == 'cifar10':\n",
        "  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "elif CURRENT_DATASET == 'cifar100':\n",
        "  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data()\n",
        "elif CURRENT_DATASET == 'mnist':\n",
        "  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "else:\n",
        "  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "print(f\"Total training examples: {len(x_train)}\")\n",
        "print(f\"Total test examples: {len(x_test)}\")\n",
        "\n",
        "inputs = np.concatenate((x_train, x_test), axis=0)\n",
        "targets = np.concatenate((y_train, y_test), axis=0)\n",
        "\n",
        "kfold = KFold(n_splits=NUM_FOLDS, shuffle=True)\n",
        "\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "precission_per_fold = []\n",
        "f1_per_fold = []\n",
        "recall_per_fold = []\n",
        "\n",
        "\n",
        "\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "  if CURRENT_METHOD == 'AUTOAUGMENT':\n",
        "    train_ds_rand = (\n",
        "        tf.data.Dataset.from_tensor_slices((inputs[train], targets[train]))\n",
        "        .shuffle(BATCH_SIZE * 100)\n",
        "        .batch(BATCH_SIZE)\n",
        "        .map(\n",
        "            lambda x, y: (tf.image.resize(x, (IMAGE_SIZE, IMAGE_SIZE)), y),\n",
        "            num_parallel_calls=AUTO,\n",
        "        )\n",
        "        .map(\n",
        "            lambda x, y: (tf.py_function(augment, [x], [tf.float32])[0], y),\n",
        "            num_parallel_calls=AUTO,\n",
        "        )\n",
        "        .prefetch(AUTO)\n",
        "    )\n",
        "\n",
        "\n",
        "    test_ds = (\n",
        "        tf.data.Dataset.from_tensor_slices((inputs[test], targets[test]))\n",
        "        .batch(BATCH_SIZE)\n",
        "        .map(lambda x, y: (tf.image.resize(x, (72, 72)), y),\n",
        "            num_parallel_calls=AUTO)\n",
        "        .prefetch(AUTO))\n",
        "\n",
        "    rand_aug_model = get_basic_training_model()\n",
        "    rand_aug_model.compile(\n",
        "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        "    )\n",
        "\n",
        "\n",
        "    rand_aug_model.fit(train_ds_rand, epochs=EPOCHS)\n",
        "\n",
        "    pred = rand_aug_model.predict(np.array([tf.image.resize(x,(72, 72)) for x in inputs[test]]))\n",
        "\n",
        "    pred = np.argmax(pred, axis=-1)\n",
        "\n",
        "    #confusion = cx(targets[test], pred)\n",
        "\n",
        "    #print(confusion)\n",
        "    report = classification_report(targets[test], pred, output_dict=True)\n",
        "\n",
        "    macro_precision =  report['macro avg']['precision']\n",
        "    macro_recall = report['macro avg']['recall']    \n",
        "    macro_f1 = report['macro avg']['f1-score']\n",
        "\n",
        "\n",
        "    scores = rand_aug_model.evaluate(test_ds)\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "    precission_per_fold.append(macro_precision)\n",
        "    recall_per_fold.append(macro_recall)\n",
        "    f1_per_fold.append(macro_f1)\n",
        "\n",
        "  elif CURRENT_METHOD == 'COMBINED':\n",
        "\n",
        "    train_ds_rand = (\n",
        "        tf.data.Dataset.from_tensor_slices((inputs[train], targets[train]))\n",
        "        .shuffle(BATCH_SIZE * 100)\n",
        "        .batch(BATCH_SIZE)\n",
        "        .map(\n",
        "            lambda x, y: (tf.image.resize(x, (IMAGE_SIZE, IMAGE_SIZE)), y),\n",
        "            num_parallel_calls=AUTO,\n",
        "        )\n",
        "        .map(\n",
        "            lambda x, y: (tf.py_function(augment, [x], [tf.float32])[0], y),\n",
        "            num_parallel_calls=AUTO,\n",
        "        )\n",
        "        .prefetch(AUTO)\n",
        "    )\n",
        "\n",
        "\n",
        "    test_ds = (\n",
        "        tf.data.Dataset.from_tensor_slices((inputs[test], targets[test]))\n",
        "        .batch(BATCH_SIZE)\n",
        "        .map(lambda x, y: (tf.image.resize(x, (72, 72)), y),\n",
        "            num_parallel_calls=AUTO)\n",
        "        .prefetch(AUTO))\n",
        "\n",
        "    rand_aug_model = get_training_model(simple_aug)\n",
        "    rand_aug_model.compile(\n",
        "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        "    )\n",
        "\n",
        "\n",
        "    rand_aug_model.fit(train_ds_rand, epochs=EPOCHS)\n",
        "\n",
        "    pred = rand_aug_model.predict(np.array([tf.image.resize(x,(72, 72)) for x in inputs[test]]))\n",
        "\n",
        "    pred = np.argmax(pred, axis=-1)\n",
        "\n",
        "    report = classification_report(targets[test], pred, output_dict=True)\n",
        "\n",
        "    macro_precision =  report['macro avg']['precision']\n",
        "    macro_recall = report['macro avg']['recall']    \n",
        "    macro_f1 = report['macro avg']['f1-score']\n",
        "\n",
        "\n",
        "    scores = rand_aug_model.evaluate(test_ds)\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "    precission_per_fold.append(macro_precision)\n",
        "    recall_per_fold.append(macro_recall)\n",
        "    f1_per_fold.append(macro_f1)\n",
        "\n",
        "  else:\n",
        "\n",
        "    simple_aug_model = get_training_model(simple_aug)\n",
        "    simple_aug_model.compile(\n",
        "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    test_ds = (\n",
        "        tf.data.Dataset.from_tensor_slices((inputs[test], targets[test]))\n",
        "        .batch(BATCH_SIZE)\n",
        "        .map(lambda x, y: (tf.image.resize(x, (72, 72)), y),\n",
        "            num_parallel_calls=AUTO)\n",
        "        .prefetch(AUTO))\n",
        "\n",
        "    simple_aug_model.fit(inputs[train], targets[train], batch_size=BATCH_SIZE, epochs=EPOCHS)\n",
        "\n",
        "    pred = simple_aug_model.predict(np.array([tf.image.resize(x,(72, 72)) for x in inputs[test]]))\n",
        "\n",
        "    pred = np.argmax(pred, axis=-1)\n",
        "\n",
        "    report = classification_report(targets[test], pred, output_dict=True)\n",
        "\n",
        "    macro_precision =  report['macro avg']['precision']\n",
        "    macro_recall = report['macro avg']['recall']    \n",
        "    macro_f1 = report['macro avg']['f1-score']\n",
        "\n",
        "\n",
        "    scores = simple_aug_model.evaluate(test_ds)\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "    precission_per_fold.append(macro_precision)\n",
        "    recall_per_fold.append(macro_recall)\n",
        "    f1_per_fold.append(macro_f1)\n",
        "\n",
        "\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%  - Precission: {precission_per_fold[i]} - Recall : {recall_per_fold[i]} - f1 : {f1_per_fold[i]}')\n",
        "\n",
        "\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print(f'> Precission : {np.mean(precission_per_fold)}')\n",
        "print(f'> Recall : {np.mean(recall_per_fold)}')\n",
        "print(f'> f1 : {np.mean(f1_per_fold)}')\n",
        "print('------------------------------------------------------------------------')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total training examples: 50000\n",
            "Total test examples: 10000\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 87s 196ms/step - loss: 1.4859 - accuracy: 0.4715\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 85s 202ms/step - loss: 1.0826 - accuracy: 0.6154\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 86s 205ms/step - loss: 0.8740 - accuracy: 0.6951\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.7399 - accuracy: 0.7430\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.6562 - accuracy: 0.7734\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.5718 - accuracy: 0.8013\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.5204 - accuracy: 0.8184\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.4735 - accuracy: 0.8345\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.4314 - accuracy: 0.8503\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.4139 - accuracy: 0.8576\n",
            "47/47 [==============================] - 3s 56ms/step - loss: 0.4756 - accuracy: 0.8413\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 90s 201ms/step - loss: 1.4543 - accuracy: 0.4766\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 86s 204ms/step - loss: 1.0577 - accuracy: 0.6242\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.8537 - accuracy: 0.7009\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.7180 - accuracy: 0.7493\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.6323 - accuracy: 0.7809\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.5694 - accuracy: 0.8015\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.5199 - accuracy: 0.8222\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.4759 - accuracy: 0.8348\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.4369 - accuracy: 0.8489\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 87s 205ms/step - loss: 0.4012 - accuracy: 0.8608\n",
            "47/47 [==============================] - 3s 57ms/step - loss: 0.5192 - accuracy: 0.8287\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 92s 206ms/step - loss: 1.4506 - accuracy: 0.4781\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 1.0533 - accuracy: 0.6276\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.8488 - accuracy: 0.7007\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.7245 - accuracy: 0.7482\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.6330 - accuracy: 0.7807\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 87s 207ms/step - loss: 0.5700 - accuracy: 0.8039\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.5203 - accuracy: 0.8204\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.4724 - accuracy: 0.8357\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.4367 - accuracy: 0.8485\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.4033 - accuracy: 0.8592\n",
            "47/47 [==============================] - 4s 57ms/step - loss: 0.6131 - accuracy: 0.8003\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 92s 206ms/step - loss: 1.4820 - accuracy: 0.4722\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 1.0531 - accuracy: 0.6267\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.8410 - accuracy: 0.7051\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.7142 - accuracy: 0.7521\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.6323 - accuracy: 0.7802\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.5713 - accuracy: 0.8033\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.5161 - accuracy: 0.8214\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.4712 - accuracy: 0.8368\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.4366 - accuracy: 0.8489\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.4017 - accuracy: 0.8600\n",
            "47/47 [==============================] - 3s 57ms/step - loss: 0.4882 - accuracy: 0.8367\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 90s 202ms/step - loss: 1.4356 - accuracy: 0.4861\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 87s 205ms/step - loss: 1.0083 - accuracy: 0.6436\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.8236 - accuracy: 0.7112\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.7055 - accuracy: 0.7551\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 87s 207ms/step - loss: 0.6214 - accuracy: 0.7841\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.5593 - accuracy: 0.8049\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.5068 - accuracy: 0.8264\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 87s 207ms/step - loss: 0.4702 - accuracy: 0.8374\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.4360 - accuracy: 0.8482\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.4012 - accuracy: 0.8616\n",
            "47/47 [==============================] - 3s 57ms/step - loss: 0.5757 - accuracy: 0.8150\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 92s 206ms/step - loss: 1.4549 - accuracy: 0.4755\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 87s 207ms/step - loss: 1.0718 - accuracy: 0.6196\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 87s 207ms/step - loss: 0.8778 - accuracy: 0.6935\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 87s 207ms/step - loss: 0.7356 - accuracy: 0.7437\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.6471 - accuracy: 0.7754\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.5796 - accuracy: 0.7997\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.5235 - accuracy: 0.8199\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.4802 - accuracy: 0.8356\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.4399 - accuracy: 0.8481\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 87s 207ms/step - loss: 0.4115 - accuracy: 0.8577\n",
            "47/47 [==============================] - 4s 58ms/step - loss: 0.5807 - accuracy: 0.7997\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 91s 203ms/step - loss: 1.4173 - accuracy: 0.4937\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 87s 207ms/step - loss: 1.0145 - accuracy: 0.6408\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 87s 207ms/step - loss: 0.8177 - accuracy: 0.7150\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 87s 207ms/step - loss: 0.7030 - accuracy: 0.7559\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 87s 207ms/step - loss: 0.6243 - accuracy: 0.7831\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 87s 207ms/step - loss: 0.5646 - accuracy: 0.8034\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.5152 - accuracy: 0.8198\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.4691 - accuracy: 0.8369\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.4287 - accuracy: 0.8519\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.4034 - accuracy: 0.8611\n",
            "47/47 [==============================] - 3s 57ms/step - loss: 0.7494 - accuracy: 0.7703\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 91s 202ms/step - loss: 1.5223 - accuracy: 0.4576\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 1.1074 - accuracy: 0.6049\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 87s 207ms/step - loss: 0.9005 - accuracy: 0.6830\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.7624 - accuracy: 0.7308\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.6586 - accuracy: 0.7726\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.5864 - accuracy: 0.7952\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.5327 - accuracy: 0.8138\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.4855 - accuracy: 0.8320\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.4501 - accuracy: 0.8449\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.4150 - accuracy: 0.8574\n",
            "47/47 [==============================] - 4s 58ms/step - loss: 0.5798 - accuracy: 0.8085\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 92s 207ms/step - loss: 1.4937 - accuracy: 0.4693\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 1.0824 - accuracy: 0.6146\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.8813 - accuracy: 0.6903\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.7467 - accuracy: 0.7379\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.6589 - accuracy: 0.7706\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.5825 - accuracy: 0.7980\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.5241 - accuracy: 0.8186\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.4811 - accuracy: 0.8339\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 87s 206ms/step - loss: 0.4457 - accuracy: 0.8447\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 87s 207ms/step - loss: 0.4061 - accuracy: 0.8591\n",
            "47/47 [==============================] - 4s 58ms/step - loss: 0.5376 - accuracy: 0.8213\n",
            "Epoch 1/10\n",
            "422/422 [==============================] - 92s 207ms/step - loss: 1.5160 - accuracy: 0.4597\n",
            "Epoch 2/10\n",
            "422/422 [==============================] - 87s 207ms/step - loss: 1.1186 - accuracy: 0.6022\n",
            "Epoch 3/10\n",
            "422/422 [==============================] - 87s 207ms/step - loss: 0.8857 - accuracy: 0.6896\n",
            "Epoch 4/10\n",
            "422/422 [==============================] - 87s 207ms/step - loss: 0.7535 - accuracy: 0.7371\n",
            "Epoch 5/10\n",
            "422/422 [==============================] - 87s 207ms/step - loss: 0.6512 - accuracy: 0.7741\n",
            "Epoch 6/10\n",
            "422/422 [==============================] - 87s 207ms/step - loss: 0.5826 - accuracy: 0.7999\n",
            "Epoch 7/10\n",
            "422/422 [==============================] - 87s 207ms/step - loss: 0.5298 - accuracy: 0.8154\n",
            "Epoch 8/10\n",
            "422/422 [==============================] - 87s 207ms/step - loss: 0.4818 - accuracy: 0.8333\n",
            "Epoch 9/10\n",
            "422/422 [==============================] - 87s 207ms/step - loss: 0.4452 - accuracy: 0.8461\n",
            "Epoch 10/10\n",
            "422/422 [==============================] - 87s 207ms/step - loss: 0.4119 - accuracy: 0.8568\n",
            "47/47 [==============================] - 3s 57ms/step - loss: 0.5276 - accuracy: 0.8198\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.4755823016166687 - Accuracy: 84.13333296775818%  - Precission: 0.8437597558944379 - Recall : 0.8423331964749743 - f1 : 0.8421914703403137\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.5192331075668335 - Accuracy: 82.86666870117188%  - Precission: 0.8348695027459951 - Recall : 0.8284169327850048 - f1 : 0.8294725102094309\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.6130502820014954 - Accuracy: 80.0333321094513%  - Precission: 0.8124905805963494 - Recall : 0.8016307621330926 - f1 : 0.8007456534468347\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.488194078207016 - Accuracy: 83.66666436195374%  - Precission: 0.8390164857688511 - Recall : 0.83779588788966 - f1 : 0.8368015953371136\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.5756804347038269 - Accuracy: 81.49999976158142%  - Precission: 0.8167520446842278 - Recall : 0.8137281890956812 - f1 : 0.8114601416281954\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Loss: 0.5806563496589661 - Accuracy: 79.96666431427002%  - Precission: 0.8322414986294959 - Recall : 0.7983097691984331 - f1 : 0.8042220764108174\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Loss: 0.749381422996521 - Accuracy: 77.03333497047424%  - Precission: 0.8154639337923033 - Recall : 0.7717880097236488 - f1 : 0.7741335994247385\n",
            "------------------------------------------------------------------------\n",
            "> Fold 8 - Loss: 0.5798044204711914 - Accuracy: 80.84999918937683%  - Precission: 0.8163014405843585 - Recall : 0.8079506868579973 - f1 : 0.8071249556792324\n",
            "------------------------------------------------------------------------\n",
            "> Fold 9 - Loss: 0.5375544428825378 - Accuracy: 82.13333487510681%  - Precission: 0.8265668958908069 - Recall : 0.8214263881154977 - f1 : 0.8189053125840955\n",
            "------------------------------------------------------------------------\n",
            "> Fold 10 - Loss: 0.5275763869285583 - Accuracy: 81.98333382606506%  - Precission: 0.8282473528389435 - Recall : 0.8201844430126697 - f1 : 0.8182739730572381\n",
            "Average scores for all folds:\n",
            "> Accuracy: 81.41666650772095 (+- 1.9732655883938968)\n",
            "> Loss: 0.5646713227033615\n",
            "> Precission : 0.8265709491425769\n",
            "> Recall : 0.814356426528666\n",
            "> f1 : 0.8143331288118011\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}